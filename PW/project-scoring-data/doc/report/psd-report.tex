\documentclass[10pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

% Add LATEX packges
\usepackage[sc]{mathpazo}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{indentfirst}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage{float}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{multirow}


% Preambula
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
%\SweaveOpts{concordance=TRUE}
\title{"German Credit" scoring data analysis report}
\author{Marta Karaś, Jan Idziak}
\date{\today}
\maketitle

% Margin settings
\newgeometry{tmargin=4cm, bmargin=4cm, lmargin=3cm, rmargin=3cm}

% Self-defined commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\textnormal{Var}}
\newcommand{\bias}{\textnormal{bias}}
\newcommand{\mse}{\textnormal{MSE}}
\newcommand{\se}{\textnormal{se}}

% Define table of content
\def\contentsname{Table of content}
\tableofcontents
\setcounter{tocdepth}{2}

% R environment global settings (source of 'config.R' file)
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## [1] "/home/martakarass/my-store/studies/PW/project-scoring-data"
\end{verbatim}
\end{kframe}
\end{knitrout}


% ==============================================================================
% ==============================================================================
% ==============================================================================
\clearpage
\part{Introduction}

In this part of the report we provide answers to the following questions about the "German Credit" data analysis we performed. 
\begin{enumerate}
\item \textit{Why was the study undertaken?}
\item \textit{What was the purpose of the research? What research questions were stated?} 
\end{enumerate}



\section{Data analysis context}

\subsection{Motivation}
This report presents results of the "German Credit" scoring data analysis which was performed as a project assignment for the "Pozyskiwanie Wiedzy" course, which we attended at Wroclaw University of Technology, Faculty of Fundamental Problems of Technology (W-11), Mathematics programm (Master) in the 2014/15 summer semester. The lecturer of the course (both lectures and laboratories) is Ph.D. Adam Zagdański.

\paragraph{}
The main goal of the project is to make use of the variety of data-mining methods we have become familiar with during the course, in order to perform complete data analysis of selected data set. We also aim to pay attention to the practical appliacnces of some parts of our work. 

\subsection{Research questions}

We stated the following research purposes for our analysis. 
\begin{enumerate}
\item Find and describe relations in the data (relations bewteen explanatory variables and response variable, relations bewteen explanatory variables). 
\item Compare different methods / algorithms to perform exploratory data analysis and predictive data analysis. 
\item Provide a summary of the analysis, containing suggestions of practical appliance and remarks regarding possible further research.  
\end{enumerate}







% ==============================================================================
% ==============================================================================
% ==============================================================================
\clearpage
\part{Materials and methods}

In this part of the report we describe the data set we obtained and the methods we use in the analysis. 

\paragraph{}
This section is rather of the decriptional / theoretical character. For a list of actual analysis steps, the outputs of the methods and more, please refer to the III part of this report.  



\section{Data set}
We perform analysis with the use of The (Statlog) German Credit Data we have obtained from the UCI Machine Learning Repository \href{http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)}{site}. 


\subsection{Data set description}
The data set contains data on 20 variables and the classification whether an applicant is considered a Good or a Bad credit risk for 1000 loan applicants. The file provided contains variables with values encoded accoring to the following schema: 

\begin{itemize}
\item Attribute 1: (qualitative) Status of existing checking account 
\newline A11 : ... < 0 DM 
\newline A12 : 0 <= ... < 200 DM 
\newline A13 : ... >= 200 DM / salary assignments for at least 1 year 
\newline A14 : no checking account 

\item Attribute 2: (numerical) Duration in month 

\item Attribute 3: (qualitative) Credit history 
\newline A30 : no credits taken/ all credits paid back duly 
\newline A31 : all credits at this bank paid back duly 
\newline A32 : existing credits paid back duly till now 
\newline A33 : delay in paying off in the past 
\newline A34 : critical account/ other credits existing (not at this bank) 

\item Attribute 4: (qualitative) Purpose 
\newline A40 : car (new) 
\newline A41 : car (used) 
\newline A42 : furniture/equipment 
\newline A43 : radio/television 
\newline A44 : domestic appliances 
\newline A45 : repairs 
\newline A46 : education 
\newline A47 : (vacation - does not exist?) 
\newline A48 : retraining 
\newline A49 : business 
\newline A410 : others 

\item Attribute 5: (numerical) Credit amount 

\item Attribute 6: (qualitative) Savings account/bonds 
\newline A61 : ... < 100 DM 
\newline A62 : 100 <= ... < 500 DM 
\newline A63 : 500 <= ... < 1000 DM 
\newline A64 : .. >= 1000 DM 
\newline A65 : unknown/ no savings account 

\item Attribute 7: (qualitative) Present employment since 
\newline A71 : unemployed 
\newline A72 : ... < 1 year 
\newline A73 : 1 <= ... < 4 years 
\newline A74 : 4 <= ... < 7 years 
\newline A75 : .. >= 7 years 

\item Attribute 8: (numerical) Installment rate in percentage of disposable income 

\item Attribute 9: (qualitative) Personal status and sex 
\newline A91 : male : divorced/separated 
\newline A92 : female : divorced/separated/married 
\newline A93 : male : single 
\newline A94 : male : married/widowed 
\newline A95 : female : single 

\item Attribute 10: (qualitative) Other debtors / guarantors 
\newline A101 : none 
\newline A102 : co-applicant 
\newline A103 : guarantor 

\item Attribute 11: (numerical) Present residence since 

\item Attribute 12: (qualitative) Property 
\newline A121 : real estate 
\newline A122 : if not A121 : building society savings agreement/ life insurance 
\newline A123 : if not A121/A122 : car or other, not in attribute 6 
\newline A124 : unknown / no property 

\item Attribute 13: (numerical) Age in years 

\item Attribute 14: (qualitative) Other installment plans 
\newline A141 : bank 
\newline A142 : stores 
\newline A143 : none 

\item Attribute 15: (qualitative) Housing 
\newline A151 : rent 
\newline A152 : own 
\newline A153 : for free 

\item Attribute 16: (numerical) Number of existing credits at this bank 

\item Attribute 17: (qualitative) Job 
\newline A171 : unemployed/ unskilled - non-resident 
\newline A172 : unskilled - resident 
\newline A173 : skilled employee / official 
\newline A174 : management/ self-employed/ highly qualified employee/ officer 

\item Attribute 18: (numerical) Number of people being liable to provide maintenance for 

\item Attribute 19: (qualitative) Telephone 
\newline A191 : none 
\newline A192 : yes, registered under the customers name 

\item Attribute 20: (qualitative) Foreign worker 
\newline A201 : yes 
\newline 202 : no 
\end{itemize}

The classification variable states whether there was a default case ('bad' client - failed to pay off the credit) or not ('good' client). 

\begin{itemize}
\item Classification: (qualitative) Default
\newline 1 (default)
\newline 0 (non-default)
\end{itemize}






% ==============================================================================
\section{Binning countinuous variables}
\paragraph{}
In credit scoring, Information Value (IV) is frequently used to compare predictive power among variables. When developing new scorecards using logistic regression, variables are often binned and recoded using WoE concept. 

\subsection{Weight of Evidence (WoE)}
One of our goals when binning variables is to maximize Information Value. Weight of Evidence (WoE) for single bin is defined as:
$$WoE = \left [ ln\left ( \frac{\text{Relative Frequency of Goods}}{\text{Relative Frequency of Bads}} \right ) \right ]\times 100.$$
We can see that value of WoE will be 0 if the odds of Relative Frequency of Goods / Relative Frequency Bads is equal to 1. If the Relative Frequency of Bads in a group is greater than the Relative Frequency of Goods, the odds ratio will be less than 1 and the WoE will be a negative number; if the Relative Frequency of Goods is greater than the Relative Frequency of Bads in a group, the WoE value will be a positive number. 

\subsection{Information Value (IV)}
We define Information Value of the variable as follow: 
$$IV = \sum_{i=1}^{k}\left [ (\text{Relative Frequency of Goods}_i-\text{Relative Frequency of Bads}_i) \times ln\left ( \frac{\text{Relative Frequency of Goods}}{\text{Relative Frequency of Bads}} \right )  \right ],$$
\paragraph{}
By convention the values of the IV statistic can be interpreted as follows. If the IV statistic is:
\begin{itemize}
\item Less than 0.02, then the predictor is not useful for modeling (separating the Goods from the Bads),
\item 0.02 to 0.1, then the predictor has only a weak relationship to the Goods/Bads odds ratio,
\item 0.1 to 0.3, then the predictor has a medium strength relationship to the Goods/Bads odds ratio,
\item 0.3 or higher, then the predictor has a strong relationship to the Goods/Bads odds ratio.
\end{itemize}

\subsection{Motivation}
The WoE recoding of predictors is particularly well suited for subsequent modeling using Logistic
Regression. Specifically, logistic regression will fit a linear regression equation of predictors (or WoE-recoded continuous predictors) to predict the logit-transformed binary Goods/Bads variable. Therefore, by using WoE-recoded predictors in logistic regression the predictors are all prepared and coded to the same WoE scale, and the parameters in the linear logistic regression
equation can be directly compared. ([8])


\clearpage
\section{Feature selection}
%In this section we describe methods for feature selection we use in our analysis. In general, we use the \href{http://cran.r-project.org/web/packages/FSelector/index.html}{FSelector} R package exhaustively. We follow [1] and this \href{http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Feature_Selection}{wiki page} to summarize algorithms from the package mentioned. 
Following [1], feature selection is essentially a task to remove irrelevant and/or redundant features. \textit{Irrelevant features} cam be removed without affecting learning performance. \textit{Redundant features} are a type of irrelevant feature. The distinction is that redundant feature implies the co-presence of another feature; individually, each feature is relevant, but the removal of one of them will not affect learning performance. 

\paragraph{}
The selection of features may be achieved in two ways:
\begin{enumerate}
\item \textbf{Feature ranking}. The idea is to rank features according to some criterion and select the top $k$ features.
\item \textbf{Subset selection}. The idea is to select a minimum subset of features without learning performance deterioration.  
\end{enumerate}
In other words, subset selection algorithms can automatically determine the number of selected features, while feature ranking algorithms need to rely on some given threshold to select features. 

\paragraph{}
The tree typical feature selection models are:
\begin{enumerate}
\item \textbf{Filter}. In a filter model, one selects the features firstly and then uses this subset to execute a classification algorithm. 
\item \textbf{Wrapper}. In a wrapper model, one employs a learning algorithm and uses its performance to determine the quality of selected features. 
\item \textbf{Embedded}. An embedded model of features selection integrates the selection of features in model builidng. An example of such model is a decision tree induction algorithm, in which at each branching node, a feature has to be selected. 
\end{enumerate}

\paragraph{}
In literature, various search strategies are proposed, including: forward, backward, floating, branch-and-bound, and randomized. A relevant issue, regarding exhaustive and heuristic searches is whether there is any reason to perform exhaustive searches if time complexity were not a concern. Research shows that exhaustive search can lead to the features that exacerbate data oerfitting, while heuristic search is less prone to data overfitting in feature selection, facing small data samples.

\paragraph{}
The evaluation of feature selection often entails two tasks: 
\begin{enumerate}
\item One is to compare two cases: before and after feature selection. The goal of this task is to observe if feature selection achieves its intended objectives. The aspects of evaluation may include the number of selected features, time, scalability and learning model's performance. 
\item The second task is to compare two feature selection algorithms to see if one is better than other for a certain task. 
\end{enumerate}



\clearpage
\subsection{Feature selection algorithms}
In this subsection we describe methods for feature selection we use in our analysis. In general, we use the \href{http://cran.r-project.org/web/packages/FSelector/index.html}{FSelector} R package exhaustively. This package contains both algorithms for filtering attributes and algorithms for wrapping classifiers and search attribute subset space. 

\subsubsection{Algorithms for filtering attributes}

\paragraph{CFS filter}
CFS is a correlation-based filter method CFS from [2]. It gives high scores to subsets that include features that are highly correlated to the class attribute but have low correlation to each other. Let $Attribute$ be an attribute subset that has $k$ attributes, $rcf$ models the correlation of the attributes to the class attribute, $rff$ - the intercorrelation between attributes. We define $Attribute$ score as:
$$CfsScore(Attribute) =\frac{k\;  rcf}{ \sqrt{k+k(k-1) rff}}.$$
The algorithm from FSelector R package makes use of \textit{Best-first search} for searching the attribute subset space. In \textit{Best-first search}, the algorithm chooses the best node from all already evaluated ones and evaluates it. The selection of the best node is repeated approximately $max.brackets$ times in case no better node found.

\paragraph{Chi-squared filter}
The algorithm evaluates the worth of an attribute by computing the value of the chi-squared statistic with respect to the class.

\paragraph{Information Gain filter}
One of the entropy-based filters. Algorithm evaluates the worth of an attribute by measuring the information gain with respect to the class.
$$InfoGain(Class, Attribute)= H(Class) +  H(Attribute) - H(Class|Attribute),$$
where $H$ is the \href{http://en.wikipedia.org/wiki/Entropy_(information_theory)}{information entropy}. 

\paragraph{Gain Ratio filter}
One of the entropy-based filters. Algorithm evaluates the worth of an attribute by measuring the gain ratio with respect to the class.
$$GainR(Class, Attribute) = \frac{H(Class) + H(Attribute) - H(Class | Attribute)}{H(Attribute)},$$
where $H$ is the information entropy. 

\paragraph{Symmetrical Uncertainty filter}
One of the entropy-based filters. Algorithm evaluates the worth of a set attributes by measuring the symmetrical uncertainty with respect to another set of attributes. 
$$SymmU(Class, Attribute)= 2\frac{H(Class) + H(Attribute) - H(Class| Attribute)}{H(Attribute) + H(Class)},$$
where $H$ is the information entropy. 

\paragraph{Linear Correlation filter}
The algorithm finds weights of continous attributes basing on their Pearson's correlation with continous class attribute.

\paragraph{Rank Correlation filter}
The algorithm finds weights of continous attributes basing on their Spearman's correlation with continous class attribute.

\paragraph{OneR algorithm}
The algorithms find weights of discrete attributes basing on very simple association rules involving only one attribute in condition part. In other words, it uses the minimum-error attribute for prediction, discretizing numeric attributes. For more information, see [4]. 

\paragraph{RReliefF filter}
The algorithm evaluates the worth of an attribute by repeatedly sampling an instance and considering the value of the given attribute for the nearest instance of the same and different class. Considering that result, it evaluates weights of attributes. Can operate on both discrete and continuous class data. For more information see [5,6,7]. 

\paragraph{Consistency-based filter}
Evaluates the worth of a subset of attributes by the level of consistency in the class values when the training instances are projected onto the subset of attributes. Consistency of any subset can never be lower than that of the full set of attributes, hence the usual practice is to use this subset evaluator in conjunction with a Random or Exhaustive search which looks for the smallest subset with consistency equal to that of the full set of attributes. The FSelector R package implementation makes use of \textit{Best-first search} for searching the attribute subset space. Works for continuous and discrete data.

\paragraph{RandomForest filter}
It is a wrapper for variable importance measure produced by randomForest algorithm. The FSelector R package implementation allows for two types of importance measure:
\begin{enumerate}
\item mean decrease in accuracy,
\item mean decrease in node impurity.
\end{enumerate}
The first measure is computed from
permuting OOB (out-of-bound) data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences. If the standard deviation of the differences is equal to 0 for a variable, the division is not done (but the average is almost always equal to 0 in that case).
\newline
The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares.


\clearpage
\subsubsection{Algorithms for wrapping classifiers and search attribute subset space}
In general, the wrapper approach depends on the so called \textit{evaluation function} that is used to return a numeric value (a score) indicating how important a given subset of features is. Typically, one uses the classification-accuracy (usually based on cross-validation) as the score for the subset. 

\paragraph{}
Below we provide a brief description of the algorithms for searching atrribute subset space. 

\paragraph{Greedy search}
At first, greedy search algorithms expand starting node, evaluate its children and choose the best one which becomes a new starting node. This process goes only in one direction. \textit{Forward search} starts from an empty and \textit{backward search} from a full set of attributes.


\paragraph{Best-first search}
The algorithm is similar to \textit{Forward search} besides the fact that is chooses the best node from all already evaluated ones and evaluates it. In the FSelector R package implementation, the selection of the best node is repeated approximately $max.brackets$ times in case no better node found. 

\paragraph{Hill climbing search}
The algorithm starts with a random attribute set. Then it evaluates all its neighbours and chooses the best one. It might be susceptible to local maximum.

\paragraph{Exhaustive search}
The algorithm searches the whole attribute subset space in breadth-first order. 







% ==============================================================================
\clearpage
\section{Classification}

\subsection{Classification algorithms}

\paragraph{kNN k- nearest neighbours}
Method is used for modeling in problem of regression or classification. 
It is simple algorithym using lazy learning. There is no actual model so all the 
computation is done while classification. 
In the problem of classification the result for every single observation is 
a class for which in k closest neighbours from the training set is the most
popular.
\paragraph{Decission trees}
Decission tree is a method that perform recussive partition of the set for every
predictor. In each step there is chosen split that separates the set between 
classes the most according to one of the meassures. The most populat measures 
are Information GAIN or GINI.
For continous data it is desired to partition variable into categorical 
(It could cause loss of the information). Result is highly corelatet with the 
learning set. Nonetheless it is easy to interpret, and attractive visually. 
Another plus is that decission trees do not have any assumptions about 
distribution of the data and algorithms works fast. 
\paragraph{Random forest}
It is curently one of the most popular method in machine learning. Its popularity 
grows thanks to good performance and small assumptions. 
However it performs well, it is hard to interpret the results, as long as model 
is complicated and consists many decission trees. 
For this method in each step of decission tree creation there is taken random 
subset of the features and then one of them is taken for split. This is done 
untli appropriate settled level. 
For Random forests the computatio time is much higher than for decission trees. 
Mostly it is because not only one tree is fitted but usualy much more. 
One of the biggest disadvantages of this model is hard interpretation of the 
output. 
Even though the subset of predictors is only taken it shows much better results
than other regulat methods for different data sets.
\paragraph{Logistic regression}
The most popular method among application in banks, insurance companies and the
industrie for modeling binary data (It could servs also for prediction 
multiclass data). It owes popularity to its simplicity, easy open form and
straight interpretation. It is subject to produce Score Card.
Method is a particular type of generalized linear model where link function has 
logit form $logit(p) = log(\frac{p}{1-p})$. It means that probability of 
occurance particular event, is modeled indirectly, as a appropriate 
transformation. 
$$ logit(p_j) = log(\frac{p_j}{1-p_j}) = \Sigma_{i=1}^{n}\beta_i X_{i,j} $$
  
Where $p_j$ is estimated probability, $\beta_i$ is factor for $X_{i,j}$ and X
represents the features of observation. 
\paragraph{Linear discriminant analysis}
It is another linear method. Under the assumption of normality and equality of 
covariance matrices within classes.
$$
  Pr(C=k|X=x) = \frac{f_k(x)\pi_k}{\Sigma_{l=1}^K f_l(x)\pi_l}
$$
  
where $C = k$ represents particular class affiliation, $x$ is observation vector 
and $f_k(x)$, has appropriate Gaussian distribution with the mean $\mu_k$ and 
covariance matrixvariance $\Sigma$ and $\pi_k$ is a-priori classes probability. 
It is enough to compare numerator as long as denominator for all classes would 
be the same.
 
\paragraph{Quadric discriminant analysis}
It is similar method to the linear dyscriminant analysis. It keeps assumption 
about normality, but in this case covarance matrices could differ. 
Aproppriate probability function keep its form: 
  
$$
  Pr(C=k|X=x) = \frac{f_k(x)\pi_k}{\Sigma_{l=1}^K f_l(x)\pi_l}
$$
As before it is enough to compare numerators for all classes.
\paragraph{Naive Bayes}
Another method that uses Bayesian rule. It is called Naive Bayes as long as it 
has a naive assumption about loss of correlation between predictors. 
However this model has easy form, it also perform well in many appliactions. 
$$
  P(Y=k|X=x) = \frac{P(X=x|Y=y)}{P(X=x)} = \frac{P(X_1=x_1,\ldots X_n=x_n|Y=y)}{P(X=x)}  
$$
  
In this case probabilities are just taken as an empirical realisation of the 
data.
It could also fall into problem of zero class frequencies. To omit this 
situation it is recommended to use one of the smoothing methods.

\clearpage
\subsection{Classification performance metrics}

\paragraph{Confussion matrix}
This is one of simple method to grade quality of the classification. It serves 
to compare acctual class of an observation to one predicted by model. 
\begin{center}
\begin{tabular}{p{1cm}c|p{1.5cm}|p{1.5cm}|c}
\cline{3-4}
& & \multicolumn{2}{ c| }{Predicted Class} \\ \cline{3-4}
& & True & False  \\ \cline{1-4}
\multicolumn{1}{ |c  }{\multirow{2}{*}{Actuall Class} } &
  \multicolumn{1}{ |c| }{True} & True Positive (TP) & False Negative (FN)  & \\ \cline{2-4}
\multicolumn{1}{ |c  }{}                        &
  \multicolumn{1}{ |c| }{False} & False Positive (FP) & True  Negative (TN) &   \\ \cline{1-4}
\end{tabular}
\end{center}
Using confussion matrix it is easier to calculate many of the goodnes of fit 
measures for the models such as sensitivity, specifity or many more. 
\paragraph{Sensitivity}
One of the simple measures called also Recall or True Positive Rate. It 
measures proportion of predicted as true and actual true. Using 
confusion matrix it could be given as:
  
  $$
  TPR = \frac{TP}{TP+FN}
  $$
\paragraph{Specifity}
This factor is also called True Negative Rate. It measures proportion predicted 
corretly as false and actuall number of false observations. It is given by:
$$
  SPC = \frac{TN}{TN+FP}
$$
  
  
\paragraph{Precission}
Popular measure in information retrival. It repressents fraction of documents
relevant to retrived. In binary classification it is defined as: 
$$
  PPV = \frac{TP}{TP+FP}
$$
  
\paragraph{False discovery rate}
It is complementary to the Precission measure that is getting more popular thanks 
to groving dimensionality of data sets. In many applications it is of an interest 
of scientist to control this factor. The formula for this coefficient is 
subsequent:
$$
  FDR = \frac{FP}{TP+FP} = 1-PPV
$$
  
\paragraph{Accuracy}
It is simple measure that could be taken as a good indicator for model 
performance. It takes proportion of all positive clasified to total
number of observations. For not equaly distributed observations between groups 
(for example in spam detection where there is many spam files classificator that
predicts everything as a spam would have high Accuracy)
$$
ACC = \frac{TP +TN}{TP+FP+TN+FN}
$$
\paragraph{F-measure}
F-measure is defined as a combination of Precission and Recall. Both of earlier
described indexes gives some information about model, but using them separtely 
can effect with falling into missclassification for specific types of data. 
$$
  F = \frac{2\cdot precision \cdot recall}{precision + recall}
$$
\subsubsection{Separation measures}
\paragraph{Kolmogorow Smirnov statistics and distributions}
Kolmogorov Smirnov statistics is the biggest distance between distribution 
functions of scores (probabilities) for accutal groups of True and False. 
Distribitions shows just simple cumulative distribution functions for classes.
\paragraph{ROC curve and GINI, AUROC indexes}
ROC or Reciver Operating Characteristic is a graphical ilustration that 
represents preformance of the binary classifier. It is being used in medicine, 
radiology, biometrics and many more applications It shows proportion of the True
Positive rate (on the vertical axis) and False Positive rate (on the horisontal 
axis) at various thresholds. 
AUC is factor strongly connected with the ROC curve. Abbreviation stands for 
area under curve. It could be calculated as follows:
$$
AUC = \int_{-\infty}^{\infty} TPR(x)FPR(x)dx
$$
\paragraph{Histogram Good vs Bad}
It is just histogram showing distribution of scores (probabilities) within 
classes. For good models there suppose to be visible difference between hight of 
the scores for different classes. 





% ==============================================================================
\clearpage
\section{Cluster analysis}

\subsection{Cluster analysis algorithms}

\subsection{Cluster analysis performance metrics}




% ==============================================================================
\clearpage
\section{Dimensionality reduction}

\subsection{Dimensionality reduction algorithms}





% ==============================================================================
% ==============================================================================
% ==============================================================================
\clearpage
\part{Results}

\clearpage
\section{Data preprocessing results}
In this section we present the results of data preprocessing we performed. The important parts of this process are: creating derived variables, binning countinuous variables and correcting bins (factor levels) if it seems reasonable. 
\paragraph{}
The procedures and methods used to perform this part of the analysis include:
\begin{itemize}
\item visual inspection of density estimator plots and fitting distribution from different distribution families to numeric variables data,
\item comparision of optimal discretization method ($smbinning$ package) and equal frequency discretization with Information Value as criterion,
\item using WoE criterion to define factor levels "similarity".
\end{itemize}


\subsection{Searching for missing, corrupt and invalid data}
\paragraph{}
We started the preprocessing in searching for missing, corrupt and invalid data. In our dataset most of the varaibles are factor variables ($PURPOSE$, $EMPLOYMENT$ etc.) Some of them are numeric but consist only of a few qunique values and thus may be seen rather like ordered factors ($NUM\_OF\_MAINTAINED\_PEOPLE$, $RESIDENCE$ etc.) We investigated frequency tables and did not notice anything anusual in the values. 
\paragraph{}
Three variables in the data set are "truly" numeric: $DURATION$, $AMOUNT$ and $AGE$. We did not find anything particularly unusual in the values of these variables. To satisfy our curiosity, we tried to fit a probabilistic distribution to the values. We used $MASS::fitdistr$ function to perform maximum-likelihood fitting of univariate distribution from selected distribution families. In each case we tried to fit parameters for three distributions families: \textit{Gamma}, \textit{Log-normal} and \textit{Weibull}. 
\paragraph{}
On the graphs below we can see kernel density estimates of variable density (black line) and curves representing density of the distribution fitted. We was not able to fit \textit{Gamma} distribution to the $AMOUNT$ variable values (\textit{Error in stats::optim(x = c(1169, 5951, 2096, 7882, 4870, 9055, 2835,  :  non-finite finite-difference value [1]}). It seems that \textit{Log-normal} distribution fits quite well in each three cases. 




\begin{figure}[h!]
\centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=.95\linewidth]{figure/unnamed-chunk-3-1} 

\end{knitrout}
\caption{Kernel density estimate of $DURATION$ variable density (black line) and curves representing density of the distribution fitted.}
\end{figure}

\begin{figure}[h!]
\centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=.95\linewidth]{figure/unnamed-chunk-4-1} 

\end{knitrout}
\caption{Kernel density estimate of $AMOUNT$ variable density (black line) and curves representing density of the distribution fitted.}
\end{figure}


\begin{figure}[h!]
\centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=.95\linewidth]{figure/unnamed-chunk-5-1} 

\end{knitrout}
\caption{Kernel density estimate of $AGE$ variable density (black line) and curves representing density of the distribution fitted.}
\end{figure}


\clearpage
\subsection{Creating derived variables}
\paragraph{}

We were considering possibilities of creating derived variables. We came up with propositions of the following formulas:
$$AMOUNT\_TO\_DURATION = AMOUNT / DURATION,$$
$$DURATION\_TO\_AGE = DURATION / AGE,$$
$$AMOUNT\_TO\_AGE = AMOUNT / AGE.$$

On the \textit{Figure 4.} we can see boxplots of $DURATION$, $AGE$ and $AMOUNT$ across two levels of response variable $RES$. On the \textit{Figure 5.} we can see boxplots of derived variables. This comparision can give us intuition how well our new variables separate good and bad bank clients.

\begin{figure}[h!]
\centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=.99\linewidth]{figure/unnamed-chunk-6-1} 

\end{knitrout}
\caption{Boxplots of $AMOUNT$, $DURATION$ and $AGE$ variables across two levels of response variable $RES$.}
\end{figure}

\begin{figure}[h!]
\centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=.99\linewidth]{figure/unnamed-chunk-7-1} 

\end{knitrout}
\caption{Boxplots of derived variables: $AMOUNT\_TO\_DURATION$, $DURATION\_TO\_AGE$ and $AMOUNT\_TO\_AGE$ across two levels of response variable $RES$.}
\end{figure}

\paragraph{}
The boxplots on the \textit{Figure 5.} agree with our intuition: we expect higher $DURATION\_TO\_AGE$ and $AMOUNT\_TO\_AGE$ values for those clients who defaulted ($RES=1$). On the other hand, we would expect the same for the $AMOUNT\_TO\_DURATION$ variable whereas the plot shows something slightly opposite. However, the value differences on the $AMOUNT\_TO\_DURATION$ boxplot are so fine that we suppose this variable is going to turn out to be of low information value and will not be consider as important one. 

\subsection{Binning continuous variables}
\paragraph{}
In our analysis we compared three approaches of dividing continuous variables into categories: 
\begin{enumerate}
\item equal frequency discretization (resulted bins are of equal number of observations),
\item supervised discretization which utilizes Recursive Partitioning to categorize the numeric characteristic and compute cutpoins based on Conditional Inference Trees algorithm ($smbinning$ package),
\item categorize variable with simple tree model (from $rpart$ package, with the use of default tree building parameters).
\end{enumerate}

The IV comparision is presented on the \textit{Figure 6.} below. Note that equal frequency discretization results are presented for:
\begin{itemize}
\item the same number of bins as in variable resulted from $smbinning$ method; signature: $equal\_nbins$,
\item the same number of bins as in variable resulted from $rpart$ method; signature: $equal\_nrparts$.
\end{itemize}



\begin{figure}[h!]
\centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=.95\linewidth]{figure/unnamed-chunk-9-1} 

\end{knitrout}
\caption{Comparision of Information Values of variables resulted from binning performed by means of 3 differend approaches. }
\end{figure}

We can see a few interesting things from the plot above: 
\begin{itemize}
\item The $smbinning$ function from the $smbinning$ seems to beat other methods in terms of IV of resulted binned variable. 
\item Derived variable $DURATION\_TO\_AGE$ has a strong relationship to the Goods/Bads odds ratio (over 0.33 IV value), whereas two other derived variables ($AMOUNT\_TO\_AGE$, $AMOUNT\_TO\_DURATION$) seem to have not. 
\end{itemize}






\clearpage
\section{Classification modelling results}

\clearpage
\section{Cluster analysis results}


\clearpage
\section{Summary}



% ==============================================================================
% ==============================================================================
% ==============================================================================
\clearpage
\part{Discussion}






% ==============================================================================
% ==============================================================================
% ==============================================================================
\clearpage
\begin{thebibliography}{99}
\bibitem{} Computational Methods of Feature Selection (Chapman \& Hall/CRC Data Mining and Knowledge Discovery Series), Huan Liu, Hiroshi Motoda, 2007, ISBN-13: 978-1584888789  
\bibitem{} Hall, M. A., Smith, L. A. (1998). Practical feature subset selection for machine learning. Australian Computer Science Conference. Springer. 181-191.
\bibitem{} Liu, H. and Setiono, R., Chi2: Feature selection and discretization of numeric attributes, Proc. IEEE 7th International Conference on Tools with Artificial Intelligence, 338-391, 1995
\bibitem{} R.C. Holte (1993). Very simple classification rules perform well on most commonly used datasets. Machine Learning. 11:63-91.
\bibitem{} Kenji Kira, Larry A. Rendell: A Practical Approach to Feature Selection. In: Ninth International Workshop on Machine Learning, 249-256, 1992.
\bibitem{} Igor Kononenko: Estimating Attributes: Analysis and Extensions of RELIEF. In: European Conference on Machine Learning, 171-182, 1994.
\bibitem{} Marko Robnik-Sikonja, Igor Kononenko: An adaptation of Relief for attribute estimation in regression. In: Fourteenth International Conference on Machine Learning, 296-304, 1997.
\bibitem{} StatSoft, Formula Guide Weight of Evidence Module http://documentation.statsoft.com/portals/0/formula%20guide/Weight%20of%20Evidence%20Formula%20Guide.pdf
\end{thebibliography}


\end{document}
